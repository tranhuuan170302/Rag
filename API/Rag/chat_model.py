from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
import json
from abc import ABC, abstractmethod
from dotenv import load_dotenv
import os
load_dotenv()

class Model(ABC):
    @abstractmethod
    def __init__(self):
        pass
    @abstractmethod
    def implement(self, **kwargs):
        pass

class Model_generator(Model):
    def __init__(self):
        self.token = os.environ.get("API_TOKEN")
        self.llm = ChatOpenAI(api_key=self.token)

    def implement(self, user_message: dict):
        """
           the function generate from LLM if the question is not in data store
       """
        prompt_classify = PromptTemplate(
            input_variables=["user_message"],
            template=f"""B·∫°n l√† m·ªôt chuy√™n gia t∆∞ v·∫•n b√°n h√†ng. Nhi·ªám v·ª• c·ªßa b·∫°n l√† 
                       **ch·ªâ ƒë·∫∑t m·ªôt c√¢u h·ªèi t∆∞∆°ng t√°c v·ªõi kh√°ch h√†ng** ƒë·ªÉ h∆∞·ªõng h·ªç v·ªÅ s·∫£n ph·∫©m ho·∫∑c 
                       lo·∫°i s·∫£n ph·∫©m ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh. Kh√¥ng c·∫ßn gi·∫£i th√≠ch d√†i d√≤ng. \n

                       ƒê·∫ßu v√†o:  
                       - **C√¢u c·ªßa kh√°ch h√†ng**: {user_message["message"]}  
                       - **Lo·∫°i c√¢u**: {user_message["category"]}  

                       ƒê·∫ßu ra:  
                       - M·ªôt c√¢u h·ªèi d·∫´n d·∫Øt kh√°ch h√†ng, v√≠ d·ª•: "B·∫°n ƒëang t√¨m s·∫£n ph·∫©m n√†o c·ª• th·ªÉ trong [lo·∫°i s·∫£n ph·∫©m]?"  

                       L∆∞u √Ω: Tr·∫£ l·ªùi ng·∫Øn g√¥n, ch·ªâ ƒë·∫∑t m·ªôt c√¢u h·ªèi ph√π h·ª£p.

                       """
        )

        formatted_prompt = prompt_classify.format(user_message=user_message)
        negotiate_classify = self.llm.invoke(formatted_prompt)

        return negotiate_classify.content


class Model_retrivele(Model):
    def __init__(self):
        self.token = os.environ.get("API_TOKEN")
        self.llm = ChatOpenAI(api_key=self.token)

    def implement(self, user_message: str, results: str) -> dict:
        """
            the function retrive knowledge in data store
        """
        if (results == ""):
            prompt_retrive = PromptTemplate(
                input_variables=["user_message"],
                template=f"""B·∫°n kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi kh√°ch h√†ng. 
                            H√£y t·∫°o m·ªôt c√¢u tr·∫£ l·ªùi chung v√† ƒë·∫∑t th√™m m·ªôt c√¢u h·ªèi ƒë·ªÉ thu th·∫≠p th√¥ng tin.  

                            C√¢u h·ªèi t·ª´ kh√°ch h√†ng:  
                            "{user_message}"  

                            Tr·∫£ l·ªùi: M·ªôt c√¢u tr·∫£ l·ªùi chung k√®m c√¢u h·ªèi d·∫´n d·∫Øt."""
            )
        else:
            prompt_retrive = PromptTemplate(
                input_variables=["user_message", "results"],
                template=f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh, th√¢n thi·ªán, v√† chuy√™n nghi·ªáp. 
                            D·ª±a tr√™n th√¥ng tin truy xu·∫•t t·ª´ c∆° s·ªü d·ªØ li·ªáu, h√£y t·∫°o m·ªôt c√¢u tr·∫£ l·ªùi **ho√†n ch·ªânh, 
                            d·ªÖ hi·ªÉu, v√† th√¢n thi·ªán v·ªõi kh√°ch h√†ng**.  

                            ### Quy t·∫Øc:  
                            1. Ch·ªâ s·ª≠ d·ª•ng d·ªØ li·ªáu truy xu·∫•t ƒë∆∞·ª£c.  
                            2. N·∫øu d·ªØ li·ªáu kh√¥ng ƒë·ªß, h√£y ƒë∆∞a ra m·ªôt c√¢u tr·∫£ l·ªùi chung v√† ƒë·∫∑t m·ªôt c√¢u h·ªèi ƒë·ªÉ d·∫´n d·∫Øt kh√°ch h√†ng cung c·∫•p th√™m th√¥ng tin.  

                            ### D·ªØ li·ªáu truy xu·∫•t:  
                            {results}  

                            ### C√¢u h·ªèi t·ª´ kh√°ch h√†ng:  
                            "{user_message}"  

                            ### Tr·∫£ l·ªùi:  
                            H√£y cung c·∫•p m·ªôt c√¢u tr·∫£ l·ªùi r√µ r√†ng, ph√π h·ª£p v·ªõi c√¢u h·ªèi kh√°ch h√†ng v√† s·ª≠ d·ª•ng th√¥ng tin t·ª´ d·ªØ li·ªáu tr√™n (lu√¥n ph·∫£i c√≥ ƒë∆∞·ªùng d·∫´n ƒë√≠nh k√®m cho kh√°ch h√†ng).

                        """)

        get_message = self.llm.invoke(prompt_retrive.format(user_message=user_message, results=results))

        # save conversation in history
        data_history = {
            "user": user_message,  # the question from user
            "assistant": get_message.content  # the answer from model
        }
        return data_history


class Model_routing(Model):

    def __init__(self):
        self.token = os.environ.get("API_TOKEN")
        self.llm = ChatOpenAI(api_key=self.token)

    def implement(self, user_message: str) -> dict:
        """
                    the function classify the question into one of the following types:
                        - Information request (Tr·∫£ v·ªÅ th√¥ng tin)
                        - Confirmation (X√°c nh·∫≠n)
                        - Descriptive (M√¥ t·∫£)
                        - Request (Y√™u c·∫ßu)
                        - Instruction (H∆∞·ªõng d·∫´n)
                    the output is a json file with the following format:
                        {
                            "type": str,
                            "message": str
                        }
                """
        prompt_classify = PromptTemplate(
            input_variables=["user_message"],
            template=f"""B·∫°n l√† m·ªôt chuy√™n gia x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n. 
                    Nhi·ªám v·ª• c·ªßa b·∫°n l√† **ph√¢n lo·∫°i c√¢u ƒë·∫ßu v√†o c·ªßa kh√°ch h√†ng v√†o m·ªôt trong c√°c ch·ªß ƒë·ªÅ d∆∞·ªõi ƒë√¢y**.  
                    C√°c ch·ªß ƒë·ªÅ g·ªìm:  
                        + Th∆∞ vi·ªán - S√°ch 
                        + Nh√† C·ª≠a - ƒê·ªùi s·ªëng
                        + ƒêi·ªán tho·∫°i - M√°y t√≠nh b·∫£ng 
                        + ƒê·ªì ch∆°i - Tr·∫ª em 
                        + Thi·∫øt b·ªã s·ªë - Ph·ª• ki·ªán s·ªë 
                        + ƒêi·ªán gia d·ª•ng 
                        + L√†m ƒë·∫πp - s·ª©c kh·ªèe 
                        + √î T√¥ - Xe ƒë·∫°p - xe m√°y 
                        + Th·ªùi trang n·ªØ
                        + B√°ch h√≥a Online
                        + Th·ªÉ Thao - Gi·∫£ ngo·∫°i 
                        + H√†ng qu·ªëc t·∫ø 
                        + Laptop - ph·ª• ki·ªán m√°y t√≠nh
                        + Kh√°c
                    ƒê·∫ßu v√†o: "{user_message}"  

                    H√£y tr·∫£ l·ªùi b·∫±ng m·ªôt trong c√°c ch·ªß ƒë·ªÅ tr√™n. Kh√¥ng c·∫ßn gi·∫£i th√≠ch. V√≠ d·ª•: kh√°c
                    """
        )

        formatted_prompt = prompt_classify.format(user_message=user_message)
        negotiate_classify = self.llm.invoke(formatted_prompt)
        data = {
            "category": negotiate_classify.content,
            "message": user_message
        }
        return data

class Model_reflection(Model):

    def __init__(self):
        self.token = os.environ.get("API_TOKEN")
        self.llm = ChatOpenAI(api_key=self.token)
        self.memory = ConversationBufferMemory(memory_key="history")

    def implement(self, history: list, user_message: str) -> str:
        clarification_prompt = PromptTemplate(
            input_variable=["history", "user_message"],
            template="""
                            B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh v√† nh·∫°y b√©n, 
                            c√≥ nhi·ªám v·ª• hi·ªÉu ng·ªØ c·∫£nh d·ª±a tr√™n to√†n b·ªô cu·ªôc h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥ ƒë·ªÉ ph·∫£n h·ªìi ch√≠nh x√°c v√† t·ª± nhi√™n.  
                            D·ª±a tr√™n c√°c tin nh·∫Øn tr∆∞·ªõc, h√£y x√°c ƒë·ªãnh √Ω ƒë·ªãnh ho·∫∑c nhu c·∫ßu c·ªßa kh√°ch h√†ng. 
                            Sau ƒë√≥, ƒë·∫∑t m·ªôt c√¢u h·ªèi ph√π h·ª£p ƒë·ªÉ d·∫´n d·∫Øt kh√°ch h√†ng ti·∫øp t·ª•c cung c·∫•p th√¥ng tin ho·∫∑c l√†m r√µ th√™m.  

                            Cu·ªôc h·ªôi tho·∫°i v·ªÅ:  
                            {history}  

                            Tin nh·∫Øn cu·ªëi c√πng t·ª´ kh√°ch h√†ng: "{user_message}"  

                            H√£y tr·∫£ l·ªùi b·∫±ng c√°ch ƒë·∫∑t m·ªôt c√¢u h·ªèi ph√π h·ª£p d·ª±a tr√™n √Ω ƒë·ªãnh trong ng·ªØ c·∫£nh cu·ªôc h·ªôi tho·∫°i. Tr·∫£ l·ªùi ng·∫Øn g√¥n, kh√¥ng gi·∫£i th√≠ch.
                    """
        )

        # agent = ReflectionAgentHistory(self.llm, self.memory)
        for entry in history:
            self.memory.chat_memory.add_user_message(entry["user"])
            if "assistant" in entry:
                self.memory.chat_memory.add_ai_message(entry["assistant"])

        history = self.memory.chat_memory.messages
        history_text = ""
        for msg in history:
            if msg.type == "human":
                history_text += f"User: {msg.content}\n"
            elif msg.type == "ai":
                history_text += f"Assistant: {msg.content}\n"

        prompt = clarification_prompt.format(history=history_text, user_message=user_message)
        clarified_message = self.llm.invoke(prompt)
        return clarified_message.content

class Model_storage(Model):
    def __init__(self):
        self.token = os.environ.get("API_TOKEN")
        self.llm = ChatOpenAI(api_key=self.token)

    def implement(self, question: str, anwser: str) -> dict:
        prompt_storage = PromptTemplate(
            input_variables = ['question', 'anwser'],
            template = f"""B·∫°n l√† m·ªôt chuy√™n gia ng√¥n ng·ªØ v√† h·ªá th·ªëng th√¥ng tin. 
                            Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë√°nh gi√° xem m·ªôt c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi 
                            c√≥ **ƒë·∫ßy ƒë·ªß th√¥ng tin** v√† **ƒë√∫ng ng·ªØ nghƒ©a** ƒë·ªÉ l∆∞u v√†o h·ªá th·ªëng d·ªØ li·ªáu hay kh√¥ng.
            
                        ### **Ti√™u ch√≠ ƒë√°nh gi√°:**
                        1. **C√¢u h·ªèi c√≥ r√µ r√†ng kh√¥ng?** (Kh√¥ng m∆° h·ªì, kh√¥ng thi·∫øu th√¥ng tin quan tr·ªçng)
                        2. **C√¢u h·ªèi c√≥ th·ªÉ hi·ªÉu theo nhi·ªÅu nghƒ©a kh√°c nhau kh√¥ng?** (N·∫øu c√≥, c·∫ßn l√†m r√µ)
                        3. **C√¢u tr·∫£ l·ªùi c√≥ ch√≠nh x√°c kh√¥ng?** (Kh√¥ng sai l·ªách, kh√¥ng ch·ª©a th√¥ng tin g√¢y hi·ªÉu l·∫ßm)
                        4. **C√¢u tr·∫£ l·ªùi c√≥ ƒë·∫ßy ƒë·ªß kh√¥ng?** (Kh√¥ng thi·∫øu √Ω ch√≠nh)
                        5. **C√¢u tr·∫£ l·ªùi c√≥ ph√π h·ª£p v·ªõi c√¢u h·ªèi kh√¥ng?** (Kh√¥ng l·∫°c ƒë·ªÅ)
                        
                        ### **Nhi·ªám v·ª• c·ªßa b·∫°n:**
                        - N·∫øu c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi **ƒë√°p ·ª©ng ƒë·ªß ti√™u ch√≠**, h√£y tr·∫£ v·ªÅ:  
                          `L∆∞u v√†o datastore: C√ì`
                        - N·∫øu **c√¢u h·ªèi ho·∫∑c c√¢u tr·∫£ l·ªùi kh√¥ng ƒë·∫°t ti√™u ch√≠**, h√£y ch·ªâ ra l·ªói sai v√† ƒë·ªÅ xu·∫•t ch·ªânh s·ª≠a.  
                        
                        ### **D·ªØ li·ªáu c·∫ßn ƒë√°nh gi√°:**  
                        - **C√¢u h·ªèi:** "{question}"  
                        - **C√¢u tr·∫£ l·ªùi:** "{anwser}"  
                        
                        üöÄ **H√£y ƒë∆∞a ra ph·∫£n h·ªìi chi ti·∫øt v√† quy·∫øt ƒë·ªãnh l∆∞u tr·ªØ!**
                        ### **ƒê·∫ßu ra d·ª± ki·∫øn: 
                        ** Cung c·∫•p k·∫øt qu·∫£ ƒë√°nh gi√° trong ƒë·ªãnh d·∫°ng JSON:
                        
                        
                            "is_valid": true/false,
                            "reason": "Gi·∫£i th√≠ch ng·∫Øn g·ªçn l√Ω do t·∫°i sao c√¢u tr·∫£ l·ªùi h·ª£p l·ªá hay kh√¥ng.",
                            "suggested_improvement": "N·∫øu c·∫ßn, h√£y ƒë·ªÅ xu·∫•t c√°ch c·∫£i thi·ªán c√¢u tr·∫£ l·ªùi."
                        
                        """
        )

        formatted_prompt = prompt_storage.format(question=question, anwser=anwser)
        result = self.llm.invoke(formatted_prompt)

        return json.loads(result.content)

model_router = Model_routing()
model_generator = Model_generator()
model_retrivel = Model_retrivele()
model_reflection = Model_reflection()
model_storage = Model_storage()
